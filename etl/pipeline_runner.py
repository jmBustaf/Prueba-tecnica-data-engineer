"""
üöÄ pipeline_runner.py ‚Äî Orquestador del pipeline ETL

Este script ejecuta el flujo completo:
1. Lee el archivo CSV por chunks.
2. Transforma cada chunk.
3. Carga los datos transformados en PostgreSQL.
4. Valida si la tabla contiene registros tras la carga.
"""

from etl.extract_csv import read_csv_chunks
from etl.transform_csv import transform_chunk
from etl.load_to_db import load_to_postgres
from etl.logger import get_logger
from etl.db import validate_table_not_empty

logger = get_logger(__name__)

def run_pipeline(filepath, table_name):
    """
    Ejecuta el pipeline completo: extracci√≥n ‚Üí transformaci√≥n ‚Üí carga ‚Üí validaci√≥n.

    Par√°metros:
    - filepath: Ruta del archivo CSV
    - table_name: Nombre de la tabla destino en PostgreSQL
    """
    try:
        for chunk in read_csv_chunks(filepath):
            df_clean = transform_chunk(chunk)
            load_to_postgres(df_clean, table_name)

        logger.info("‚úÖ Pipeline finalizado correctamente.")

        # Validaci√≥n post-carga
        if validate_table_not_empty(table_name):
            logger.info(f"üìä La tabla '{table_name}' fue cargada exitosamente.")
        else:
            logger.warning(f"‚ö†Ô∏è La tabla '{table_name}' est√° vac√≠a tras la carga.")

    except Exception as e:
        logger.error(f"‚ùå Error en el pipeline: {str(e)}")
